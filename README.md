# Traffic Accidents ETL con Kafka

## DescripciÃ³n General

El proyecto **Traffic Accidents ETL con Kafka** es una soluciÃ³n avanzada de pipeline de datos, diseÃ±ada para procesar, analizar y visualizar datos de accidentes de trÃ¡fico en tiempo real. Integra **Apache Airflow** para la orquestaciÃ³n de flujos de trabajo, **Kafka** para el streaming de datos, y **Streamlit** para la visualizaciÃ³n en tiempo real. El pipeline extrae datos de un CSV de **Kaggle** (migrado a **PostgreSQL**) y los enriquece con informaciÃ³n geoespacial de **OpenStreetMap (OSM)**, permitiendo anÃ¡lisis de patrones de accidentes y zonas de riesgo. Los datos transformados se almacenan en un modelo dimensional y se visualizan mediante dashboards en **Power BI** y **Streamlit**.

---

## Objetivos

* **Extraer**: Obtener datos de accidentes de trÃ¡fico desde PostgreSQL (`CrashTraffic`) y datos geoespaciales desde la API de OSM.
* **Transformar**: Limpiar, enriquecer y estructurar los datos dentro de un modelo dimensional.
* **Cargar**: Almacenar los datos transformados en PostgreSQL (`CrashTraffic_Dimensional`).
* **Orquestar**: Automatizar el pipeline utilizando Apache Airflow.
* **Analizar**: Realizar anÃ¡lisis exploratorios (EDA) sobre datos de accidentes y de OSM.
* **Visualizar**: Crear dashboards en Power BI y Streamlit.
* **Streaming**: Transmitir mÃ©tricas con Kafka y visualizarlas en tiempo real mediante Streamlit.

---

## Fuente de Datos

El dataset principal proviene de **Kaggle** (*Traffic Accidents*), con mÃ¡s de **10,000 registros** sobre severidad, clima y datos temporales, migrado a PostgreSQL (`CrashTraffic`). Este se enriquece con datos geoespaciales de **OSM**.

---

## Pipeline ETL

### ExtracciÃ³n

* **Fuente**: Tabla `CrashTraffic` en PostgreSQL y API de OSM.
* **MÃ©todo**: Scripts en Python y consultas SQL; OSM procesado en `API_EDA.ipynb`.

### TransformaciÃ³n

* **Limpieza**: EliminaciÃ³n de nulos y duplicados, estandarizaciÃ³n de categorÃ­as.
* **IngenierÃ­a**: ConversiÃ³n de fechas y creaciÃ³n de variables categÃ³ricas/binarias.
* **Enriquecimiento**: IntegraciÃ³n con datos de OSM.
* **Modelo Dimensional**: Esquema en estrella para consultas eficientes.
* **Pruebas Unitarias**: ValidaciÃ³n de transformaciones con `pytest`.
* **Great Expectations**: ValidaciÃ³n de datos antes del merge.
* **Merge**: CombinaciÃ³n de datos entre la base y OSM.

### Carga

* **Destino**: `CrashTraffic_Dimensional` en PostgreSQL.
* **MÃ©todo**: Carga mediante SQLAlchemy vÃ­a Airflow.

### OrquestaciÃ³n

* **Airflow**: DAG `etl_crash_traffic` para automatizar el proceso ETL.

### Streaming

* **Kafka**: TransmisiÃ³n de mÃ©tricas post-merge.
* **Consumidor**: Script en Python con Streamlit para visualizaciÃ³n en tiempo real.

---

## AnÃ¡lisis Exploratorio (EDA)

* **API EDA (`API_EDA.ipynb`)**: AnÃ¡lisis de datos desde OSM.
* **Dataset EDA (`002_EDA_csv.ipynb`)**: AnÃ¡lisis univariado, bivariado y temporal de accidentes.

---

## Visualizaciones

* **Power BI**: Dashboard con insights clave (severidad, clima, tendencias).
* **Streamlit**: Dashboard en tiempo real mostrando mÃ©tricas vÃ­a Kafka.

---

## Project Structure

```
ðŸ“‚ Traffic-Accidents-ETL
TRAFFICACCIDENTS/
â”œâ”€â”€ API/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ API_EDA.ipynb
â”‚   â”‚   â””â”€â”€ git_attributes
â”œâ”€â”€ config/
â”‚   â””â”€â”€ conexion_db.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_crash_traffic.py
â”œâ”€â”€ Dashboard/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ CrashTraffic_clean.csv
â”‚   â””â”€â”€ CrashTraffic.csv
â”œâ”€  logs/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 001_extract.ipynb
â”‚   â”œâ”€â”€ 002_EDA_csv.ipynb
â”œâ”€â”€ pdf/
â”œâ”€â”€ venv/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## TecnologÃ­as

* **Lenguaje y procesamiento**: Python, pandas, numpy
* **VisualizaciÃ³n**: matplotlib, seaborn, Power BI, Streamlit
* **Base de Datos**: PostgreSQL, psycopg2, SQLAlchemy
* **OrquestaciÃ³n**: Apache Airflow
* **Streaming**: Apache Kafka
* **ContenerizaciÃ³n**: Docker, Docker Compose
* **Notebooks**: Jupyter
* **Versionado**: Git, GitHub
* **Pruebas**: pytest
* **ValidaciÃ³n**: Great Expectations

---

## Destino Final

* **Base de Datos**: `CrashTraffic_Dimensional`
* **Dashboards**: Power BI y Streamlit

---

## Rendimiento y Hallazgos

* **Eficiencia**: OptimizaciÃ³n para grandes volÃºmenes de datos.
* **Hallazgos**: Correlaciones entre condiciones adversas y lesiones, identificaciÃ³n de picos de accidentes y zonas de alto riesgo.

---

## Requerimientos Cumplidos

Cumple con los criterios del proyecto **"ETL class project - Final delivery"**:

* Fuentes: Kaggle y OSM
* DAG Airflow: ETL con modelo dimensional
* Streaming: Kafka para mÃ©tricas
* Dashboards: Power BI y Streamlit
* Consumidor Kafka: VisualizaciÃ³n en Streamlit
* GitHub: Repositorio con notebooks de EDA incluidos

---

## InstalaciÃ³n

1. **Clonar el repositorio**:

   ```bash
   git clone https://github.com/isabellaperezcav/Traffic-Accidents-Kafka.git
   cd Traffic-Accidents-Kafka
   ```

2. **Crear entorno virtual**:

   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

3. **Instalar dependencias**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar PostgreSQL**:

   ```sql
   CREATE DATABASE crash_traffic;
   CREATE DATABASE crash_traffic_dimensional;
   ```

   Actualizar `config/conexion_db.py`.

5. **Variables de entorno**: Crear archivo `.env` con:

   ```
   AIRFLOW__CORE__EXECUTOR=SequentialExecutor
   AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@localhost:5432/airflow
   DB_HOST=localhost
   DB_PORT=5432
   DB_USER=tu_usuario
   DB_NAME=tu_base_de_datos
   DB_PASSWORD=tu_contraseÃ±a
   KAFKA_BOOTSTRAP_SERVERS=localhost:9092
   ```

6. **Levantar Airflow con Docker**:

   ```bash
   docker-compose up -d
   ```

   Acceder a: [http://localhost:8080](http://localhost:8080)

7. **Configurar Kafka**: Crear topic y verificar ejecuciÃ³n.

8. **Ejecutar notebooks**:

   * `001_extract.ipynb`
   * `002_EDA_csv.ipynb`
   * `API_EDA.ipynb`

9. **Ejecutar pipeline**: Activar DAG `etl_crash_traffic` en Airflow.

10. **Kafka y Streamlit**:

    ```bash
    python kafka/producer.py
    python streamlit/app.py
    ```

---

## Uso

* **Pipeline**: Activar DAG en Airflow.
* **Dashboards**: Explorar visualizaciones en Power BI y Streamlit tras la ejecuciÃ³n.

---

## Mantenimiento

* **Monitoreo**: Desde la interfaz de Airflow.
* **Problemas comunes**:

  * ConexiÃ³n: Verificar credenciales.
  * Airflow: Revisar dependencias.
  * Kafka: Asegurar creaciÃ³n de topic y ejecuciÃ³n del productor/consumidor.

---

## ConclusiÃ³n

Este pipeline robusto integra herramientas avanzadas para ofrecer insights sobre accidentes de trÃ¡fico, apoyando estrategias de **seguridad vial** a travÃ©s del anÃ¡lisis de datos en tiempo real.
